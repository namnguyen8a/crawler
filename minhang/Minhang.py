import requests
import pandas as pd
import os
import re
import time
from openpyxl import load_workbook
import json


# --- Cáº¤U HÃŒNH ---
# API endpoint cá»§a trang web
api_url = r'https://ss.shanghai.gov.cn/manda-app/api/app/search/v1/vgyfvp/search'

# CÃ¡c biáº¿n Ä‘iá»u khiá»ƒn quÃ¡ trÃ¬nh crawl
request_delay = 0.5  # GiÃ¢y, thá»i gian chá» giá»¯a cÃ¡c request
retry_attempts = 3   # Sá»‘ láº§n thá»­ láº¡i náº¿u request tháº¥t báº¡i
retry_delay = 5      # GiÃ¢y, thá»i gian chá» trÆ°á»›c khi thá»­ láº¡i

# TÃªn cÃ¡c file output vÃ  checkpoint
checkpoint_file = 'checkpoint.log'
txt_filename = 'crawled_titles.txt'
excel_filename = 'æ¨¡ç‰ˆ.xlsx'
sheet_name_to_update = 'shanghai'

# Headers Ä‘á»ƒ giáº£ máº¡o lÃ  má»™t trÃ¬nh duyá»‡t há»£p lá»‡
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 QuarkPC/4.6.5.580',
    'Content-Type': 'application/json',
    'Accept': 'application/json, text/plain, */*',
    'Origin': 'https://www.shmh.gov.cn/',
    'Referer': 'https://www.shmh.gov.cn/',
}

# --- Cáº¤U HÃŒNH Tá»ª KHÃ“A ---

# Bá»˜ Lá»ŒC: TiÃªu Ä‘á» Báº®T BUá»˜C pháº£i chá»©a Ã­t nháº¥t má»™t trong cÃ¡c tá»« khÃ³a nÃ y
ADDRESS_INDICATOR_KEYWORDS = ["åŠ è£…ç”µæ¢¯å·¥ç¨‹"]

# Bá»˜ LÃ€M Sáº CH: CÃ¡c tá»« thá»«a á»Ÿ Äáº¦U hoáº·c CUá»I tiÃªu Ä‘á» sáº½ bá»‹ xÃ³a bá»
CLEANUP_KEYWORDS = [
    "é¡¹ç›®", "åœ°å—", "å·¥ç¨‹", "æ–¹æ¡ˆ", "è§„åˆ’", "è®¾è®¡", "å»ºè®¾", "åŠ è£…ç”µæ¢¯"
    "æ–°å»º", "æ”¹å»º", "ä¿®ç¼®", "æ‰©å»º", "æ”¹é€ ", "ç”¨æˆ¿", "å…¬ç¤º", "å…¬å‘Š", "ä¸¾åŠ", "æœ‰å…³å†…å®¹äºˆä»¥å…¬ç¤º", "å·¥ç¨‹é¡¹ç›®æ‹ŸæŠ¥", "å·¥ç¨‹é¡¹ç›®"
    "é¢„å…¬å‘Š", "å…³äº", "ï¼ˆå·²ç»“æŸï¼‰", "çš„æ„è§å¾è¯¢", "åŠ è£…ç”µæ¢¯", "æ—¢æœ‰å¤šå±‚ä½å®…", "å¬å¼€", "ä¸“ç ”", "è°ƒç ”", "å–œ", "å¥½æ¶ˆæ¯", "æ–¹æ¡ˆå…¬ç¤º"
]


# --- CÃC HÃ€M Há»– TRá»¢ ---

def find_last_row_with_data(sheet):
    """QuÃ©t ngÆ°á»£c tá»« dÆ°á»›i lÃªn Ä‘á»ƒ tÃ¬m hÃ ng cuá»‘i cÃ¹ng thá»±c sá»± cÃ³ dá»¯ liá»‡u."""
    for row in range(sheet.max_row, 0, -1):
        for cell in sheet[row]:
            if cell.value is not None:
                return row
    return 0


def parse_title_hybrid_improved(title_text):
    """
    HÃ m phÃ¢n tÃ­ch tiÃªu Ä‘á» Ä‘Ã£ Ä‘Æ°á»£c cáº£i tiáº¿n:
    1. XÃ³a cÃ¡c tá»« khÃ³a khÃ´ng mong muá»‘n á»Ÿ Ä‘áº§u chuá»—i.
    2. Cáº¯t bá» pháº§n Ä‘uÃ´i khÃ´ng mong muá»‘n báº¯t Ä‘áº§u báº±ng má»™t tá»« khÃ³a.
    3. TÃ¬m tá»« khÃ³a Ä‘á»‹a chá»‰ Ä‘á»ƒ cáº¯t chuá»—i chÃ­nh xÃ¡c.
    """
    address = title_text.strip()

    # --- BÆ¯á»šC 1: XÃ³a cÃ¡c tá»« khÃ³a thá»«a á»Ÿ Äáº¦U chuá»—i ---
    is_prefix_cleaned = True
    while is_prefix_cleaned:
        is_prefix_cleaned = False

        # FIRST: Remove location prefixes (ä¸Šæµ·å¸‚, é™å®‰åŒº)
        location_prefixes = ["ä¸Šæµ·å¸‚é—µè¡ŒåŒº", "ä¸Šæµ·é—µè¡ŒåŒº", "é—µè¡ŒåŒº", "ä¸Šæµ·å¸‚", "ä¸Šæµ·", "é—µè¡Œ"]
        for location in location_prefixes:
            if address.startswith(location):
                address = address[len(location):].strip(' :ï¼š')
                is_prefix_cleaned = True
                print(f"  ğŸ—‘ï¸ ÄÃ£ xÃ³a Ä‘á»‹a danh: '{location}'")
                break

        # ONLY if no location was removed, then check CLEANUP_KEYWORDS
        if not is_prefix_cleaned:
            for keyword in CLEANUP_KEYWORDS:
                if address.startswith(keyword):
                    address = address[len(keyword):].strip(' :ï¼š')
                    is_prefix_cleaned = True
                    print(f"  ğŸ§¹ ÄÃ£ xÃ³a tá»« khÃ³a: '{keyword}'")
                    break

    # --- BÆ¯á»šC 2: Cáº¯t bá» pháº§n Ä‘uÃ´i thá»«a ---
    min_pos = -1
    for keyword in CLEANUP_KEYWORDS:
        pos = address.find(keyword)
        if pos > 0 and (min_pos == -1 or pos < min_pos):
            min_pos = pos

    if min_pos != -1:
        address = address[:min_pos].strip()

    # --- BÆ¯á»šC 3: TÃ¬m vÃ  cáº¯t táº¡i tá»« khÃ³a Ä‘á»‹a chá»‰ (FIXED PART) ---
    address = address.replace('ï¼ˆæš‚åï¼‰', '').strip()

    # Search for ALL address keywords, not just "å·"
    address_keywords = ["å·", "å·æ¥¼", "å•å…ƒ", "å¹¢ï¼‰", "æ¥¼", "å¼„", "å®…æ¥¼", "å¹¢", "è¡—é“", "å°åŒº", "ä¸­å­¦", "å°å­¦", "è¡—åŠ", "æ‘", "è‹‘", "é™¢"]
    found_pos = -1
    found_keyword = None

    for keyword in address_keywords:
        pos = address.rfind(keyword)
        if pos != -1:
            # Calculate where to cut (end of the keyword)
            cut_position = pos + len(keyword)
            # Only use this if it's further right than previous finds
            if cut_position > found_pos:
                found_pos = cut_position
                found_keyword = keyword

    # If we found any address keyword, cut the string there
    if found_pos != -1 and found_keyword:
        address = address[:found_pos].strip()
        print(f"  âœ… ÄÃ£ cáº¯t táº¡i tá»« khÃ³a: '{found_keyword}'")

    return "é—µè¡ŒåŒº", address

def read_checkpoint():
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            try:
                content = f.read().strip()
                return int(content) if content else 0
            except (ValueError, IndexError):
                return 0
    return 0

def write_checkpoint(page_num):
    with open(checkpoint_file, 'w') as f:
        f.write(str(page_num))

# --- PHáº¦N CHÃNH: THU THáº¬P VÃ€ Xá»¬ LÃ Dá»® LIá»†U ---
try:
    all_extracted_data = []
    all_raw_titles = []

    # --- Láº¥y tá»•ng sá»‘ trang má»™t cÃ¡ch tá»± Ä‘á»™ng ---
    print("Äang kiá»ƒm tra tá»•ng sá»‘ trang...")
    initial_payload = {
        "cid": "dOIsK2GVTQfYOJS4EeNHfiaJWNcCku7x", "uid": "dOIsK2GVTQfYOJS4EeNHfiaJWNcCku7x", "query": "åŠ è£…ç”µæ¢¯å·¥ç¨‹",
        "current": 1, "size": 20, "disable_correction": False,
        "facets": {"view": [{"type": "value", "name": "view", "sort": {"count": "desc"}, "size": 10}]},
        "input_type": "Input"
    }

    try:
        initial_response = requests.post(api_url, headers=headers, json=initial_payload, timeout=30)
        initial_response.raise_for_status()
        initial_data = initial_response.json()
        total_pages = initial_data.get('result', {}).get('_meta', {}).get('page', {}).get('total_pages', 0)
        if total_pages == 0:
            print("KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ hoáº·c khÃ´ng thá»ƒ láº¥y Ä‘Æ°á»£c tá»•ng sá»‘ trang.")
            exit()
        print(f"PhÃ¡t hiá»‡n tháº¥y cÃ³ tá»•ng cá»™ng {total_pages} trang káº¿t quáº£.")
    except requests.exceptions.RequestException as e:
        print(f"Lá»—i nghiÃªm trá»ng: KhÃ´ng thá»ƒ láº¥y thÃ´ng tin tá»•ng sá»‘ trang. Lá»—i: {e}")
        exit()

    # --- Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh crawl ---
    last_completed_page = read_checkpoint()
    start_page = last_completed_page + 1

    if start_page > total_pages:
        print(f"Checkpoint cho tháº¥y Ä‘Ã£ thu tháº­p xong {last_completed_page}/{total_pages} trang. KhÃ´ng cáº§n cháº¡y láº¡i.")
    else:
        if start_page > 1:
            print(f"ÄÃ£ phÃ¡t hiá»‡n checkpoint. Tiáº¿p tá»¥c tá»« trang {start_page}...")

        for page_num in range(start_page, total_pages + 1):
            payload = initial_payload.copy()
            payload['current'] = page_num
            print(f"--- Äang thu tháº­p trang {page_num}/{total_pages} tá»« API ---")

            response = None
            for attempt in range(retry_attempts):
                try:
                    response = requests.post(api_url, headers=headers, json=payload, timeout=30)
                    response.raise_for_status()
                    break
                except requests.exceptions.RequestException as req_err:
                    print(f"  Lá»—i káº¿t ná»‘i á»Ÿ trang {page_num} (láº§n {attempt + 1}/{retry_attempts}): {req_err}")
                    if attempt < retry_attempts - 1:
                        time.sleep(retry_delay)
                    else:
                        print("  ÄÃ£ háº¿t sá»‘ láº§n thá»­ láº¡i. Bá» qua trang nÃ y.")

            if response is None: continue

            try:
                data = response.json()
                results = data.get('result', {}).get('items', [])
            except json.JSONDecodeError:
                print(f"  KhÃ´ng thá»ƒ phÃ¢n tÃ­ch JSON tá»« trang {page_num}. Bá» qua."); continue

            if not results: print(f"Trang {page_num} khÃ´ng cÃ³ dá»¯ liá»‡u. CÃ³ thá»ƒ Ä‘Ã£ Ä‘áº¿n trang cuá»‘i."); break

            for item in results:
                full_title = item.get('title', {}).get('raw', '').strip()

                # BÆ¯á»šC 1: Lá»ŒC. Chá»‰ xá»­ lÃ½ náº¿u tiÃªu Ä‘á» chá»©a tá»« khÃ³a Ä‘á»‹a chá»‰.
                if not any(keyword in full_title for keyword in ADDRESS_INDICATOR_KEYWORDS):
                    print(f"  -> Bá» qua (khÃ´ng cÃ³ tá»« khÃ³a Ä‘á»‹a chá»‰): {full_title}")
                    continue

                # BÆ¯á»šC 2: Xá»¬ LÃ. Náº¿u Ä‘Ã£ qua bá»™ lá»c, tiáº¿n hÃ nh lÃ m sáº¡ch vÃ  trÃ­ch xuáº¥t.
                date_string = item.get('date', {}).get('raw', '').strip()

                if full_title and date_string:
                    all_raw_titles.append(full_title)
                    district, address = parse_title_hybrid_improved(full_title) # Gá»i hÃ m lÃ m sáº¡ch nÃ¢ng cao
                    parts = date_string.split('-')
                    year, month, day = (parts[0], parts[1], parts[2]) if len(parts) == 3 else ("", "", "")
                    all_extracted_data.append({
                        'åŒº': district, 'åœ°å€': address, 'å¹´': year, 'æœˆ': month, 'æ—¥': day
                    })

            write_checkpoint(page_num)
            print(f"  ÄÃ£ xá»­ lÃ½ xong trang {page_num}. ÄÃ£ lÆ°u checkpoint.")
            time.sleep(request_delay)

        # --- Ghi dá»¯ liá»‡u ra file ---
        print(f"\n>>> Thu tháº­p hoÃ n táº¥t. Tá»•ng cá»™ng {len(all_extracted_data)} tiÃªu Ä‘á» há»£p lá»‡ Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trong láº§n cháº¡y nÃ y. <<<\n")

        if all_raw_titles:
            with open(txt_filename, 'a', encoding='utf-8') as f:
                f.write('\n'.join(all_raw_titles) + '\n')
            print(f"ÄÃ£ thÃªm {len(all_raw_titles)} tiÃªu Ä‘á» thÃ´ vÃ o file '{txt_filename}'")

        if all_extracted_data:
            columns_order = ['åŒº', 'åœ°å€', 'å¹´', 'æœˆ', 'æ—¥']
            new_df = pd.DataFrame(all_extracted_data)[columns_order]
            try:
                book = load_workbook(excel_filename)
                sheet = book[sheet_name_to_update] if sheet_name_to_update in book.sheetnames else book.create_sheet(sheet_name_to_update)
                last_data_row = find_last_row_with_data(sheet)
                start_row = last_data_row + 1

                # Ghi header náº¿u lÃ  sheet má»›i
                if start_row <= 1:
                    headers_list = list(new_df.columns)
                    for col_idx, header_value in enumerate(headers_list, start=2): sheet.cell(row=1, column=col_idx, value=header_value)
                    start_row = 2 # Dá»¯ liá»‡u sáº½ báº¯t Ä‘áº§u tá»« hÃ ng 2

                print(f"Äang ghi {len(new_df)} dÃ²ng dá»¯ liá»‡u má»›i vÃ o sheet '{sheet_name_to_update}' báº¯t Ä‘áº§u tá»« hÃ ng {start_row}...")
                for i, row_data in new_df.iterrows():
                    current_row = start_row + i
                    for col_idx, col_name in enumerate(columns_order, start=2):
                        sheet.cell(row=current_row, column=col_idx, value=row_data[col_name])

                book.save(excel_filename)
                print("HoÃ n táº¥t! File Excel Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t thÃ nh cÃ´ng.")
            except FileNotFoundError: print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file Excel '{excel_filename}'.")
            except Exception as ex: print(f"Lá»—i khi ghi file Excel: {ex}")
        else:
            print("KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i há»£p lá»‡ Ä‘á»ƒ ghi vÃ o file Excel.")
except Exception as e:
    print(f"ÄÃ£ xáº£y ra má»™t lá»—i khÃ´ng mong muá»‘n: {e}")