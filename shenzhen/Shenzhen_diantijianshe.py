import requests
import pandas as pd
import os
import re
import time
from openpyxl import load_workbook
import json

# --- Cáº¤U HÃŒNH ---
# API endpoint chÃ­nh xÃ¡c
api_url = 'https://search.gd.gov.cn/api/search/all'

# CÃ¡c biáº¿n Ä‘iá»u khiá»ƒn quÃ¡ trÃ¬nh crawl
request_delay = 0.5
retry_attempts = 3
retry_delay = 5

# TÃªn cÃ¡c file output vÃ  checkpoint
checkpoint_file = 'checkpoint_shenzhen.log'
txt_filename = 'crawled_titles_shenzhen.txt'
excel_filename = '251107_SH Crawled Data File.xlsx'
sheet_name_to_update = 'shenzhen'

# Headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Content-Type': 'application/json',
    'Accept': 'application/json, text/plain, */*',
    'Origin': 'https://search.gd.gov.cn',
    'Referer': 'https://search.gd.gov.cn/',
}

# --- Cáº¤U HÃŒNH Tá»ª KHÃ“A ---
ADDRESS_INDICATOR_KEYWORDS = ["ç”µæ¢¯å»ºè®¾", "ç”µæ¢¯å·¥ç¨‹", "ç”µæ¢¯æ€»å¹³"]
CLEANUP_KEYWORDS = [
    "é¡¹ç›®", "åœ°å—", "å·¥ç¨‹", "æ–¹æ¡ˆ", "è§„åˆ’", "è®¾è®¡", "å»ºè®¾", "æ·±åœ³å¸‚", "å—å±±åŒº", "è®¾è®¡æ–¹æ¡ˆ", "æ€»å¹³é¢",
    "æ–°å»º", "æ”¹å»º", "ä¿®ç¼®", "æ‰©å»º", "æ”¹é€ ", "ç”¨æˆ¿", "å…¬ç¤º", "å…¬å‘Š", "å…³äº", "é€šå‘Š", "åŠ å»ºç”µæ¢¯", "æ–°å¢",
    "æ‰¹å‰å…¬ç¤º", "è§„åˆ’è®¸å¯", "ç®¡ç†å±€", "å¸‚è§„åˆ’å’Œè‡ªç„¶èµ„æºå±€", "å—å±±ç®¡ç†å±€", "æ–°å¢ç”µæ¢¯å·¥ç¨‹æ€»å¹³é¢å›¾ä¿®æ”¹çš„å…¬ç¤º", "åŠ å»ºç”µæ¢¯æ€»å¹³é¢å›¾çš„é€šå‘Š"
]

# Danh sÃ¡ch cÃ¡c quáº­n táº¡i ThÃ¢m Quyáº¿n
SHENZHEN_DISTRICTS = [
    "å—å±±åŒº", "ç¦ç”°åŒº", "ç½—æ¹–åŒº", "ç›ç”°åŒº", "å®å®‰åŒº", "é¾™å²—åŒº", "é¾™ååŒº", "åªå±±åŒº", "å…‰æ˜åŒº", "å¤§é¹æ–°åŒº"
]

# Mapping tá»« khÃ³a Ä‘á»‹a danh Ä‘áº¿n quáº­n
DISTRICT_KEYWORDS = {
    "å—å±±åŒº": ["å—å±±", "è›‡å£", "åä¾¨åŸ", "ç§‘æŠ€å›­", "åæµ·", "å‰æµ·", "è¥¿ä¸½", "ç²¤æµ·", "æ²™æ²³"],
    "ç¦ç”°åŒº": ["ç¦ç”°", "åå¼ºåŒ—", "ä¸­å¿ƒåŒº", "çš‡å²—", "è²èŠ±å±±", "é¦™èœœæ¹–", "æ¢…æ—", "å›­å²­"],
    "ç½—æ¹–åŒº": ["ç½—æ¹–", "ä¸œé—¨", "å›½è´¸", "ç«è½¦ç«™", "è²å¡˜", "é»„è´", "æ¡‚å›­", "ç¬‹å²—"],
    "ç›ç”°åŒº": ["ç›ç”°", "å¤§æ¢…æ²™", "å°æ¢…æ²™", "æ²™å¤´è§’", "æµ·å±±"],
    "å®å®‰åŒº": ["å®å®‰", "è¥¿ä¹¡", "ç¦æ°¸", "æ²™äº•", "æ¾å²—", "çŸ³å²©", "æ–°å®‰", "èˆªåŸ"],
    "é¾™å²—åŒº": ["é¾™å²—", "å¸ƒå‰", "æ¨ªå²—", "å¹³æ¹–", "å‚ç”°", "å—æ¹¾", "å›­å±±", "å®é¾™"],
    "é¾™ååŒº": ["é¾™å", "è§‚æ¾œ", "æ°‘æ²»", "å¤§æµª", "ç¦åŸ", "è§‚æ¹–"],
    "åªå±±åŒº": ["åªå±±", "å‘æ¢“", "é¾™ç”°", "çŸ³äº•", "é©¬å³¦"],
    "å…‰æ˜åŒº": ["å…‰æ˜", "å…¬æ˜", "æ–°æ¹–", "å‡¤å‡°", "ç‰å¡˜", "é©¬ç”°"],
    "å¤§é¹æ–°åŒº": ["å¤§é¹", "è‘µæ¶Œ", "å—æ¾³", "å¤§é¹è¡—é“", "è‘µæ¶Œè¡—é“"]
}


# --- CÃC HÃ€M Há»– TRá»¢ ---
def find_last_row_with_data(sheet):
    """QuÃ©t ngÆ°á»£c tá»« dÆ°á»›i lÃªn Ä‘á»ƒ tÃ¬m hÃ ng cuá»‘i cÃ¹ng thá»±c sá»± cÃ³ dá»¯ liá»‡u."""
    for row in range(sheet.max_row, 0, -1):
        for col in range(1, sheet.max_column + 1):
            if sheet.cell(row, col).value is not None:
                return row
    return 0


def extract_district_from_title(title):
    """
    TrÃ­ch xuáº¥t quáº­n trá»±c tiáº¿p tá»« tiÃªu Ä‘á» (Æ°u tiÃªn cao nháº¥t)
    """
    # TÃ¬m quáº­n trá»±c tiáº¿p trong tiÃªu Ä‘á»
    for district in SHENZHEN_DISTRICTS:
        if district in title:
            return district

    # TÃ¬m tá»« khÃ³a quáº­n trong tiÃªu Ä‘á» (vÃ­ dá»¥: "ç¦ç”°ç®¡ç†å±€" -> "ç¦ç”°åŒº")
    for district, keywords in DISTRICT_KEYWORDS.items():
        for keyword in keywords:
            # TÃ¬m pattern nhÆ° "ç¦ç”°ç®¡ç†å±€", "å—å±±ç®¡ç†å±€", etc.
            if f"{keyword}ç®¡ç†å±€" in title:
                return district
            # TÃ¬m tá»« khÃ³a Ä‘Æ¡n láº»
            if keyword in title and len(keyword) > 1:  # TrÃ¡nh trÃ¹ng láº·p vá»›i tá»« ngáº¯n
                # Kiá»ƒm tra xem tá»« khÃ³a cÃ³ pháº£i lÃ  má»™t tá»« Ä‘á»™c láº­p khÃ´ng
                pattern = r'[^a-zA-Z0-9]' + re.escape(keyword) + r'[^a-zA-Z0-9]'
                if re.search(pattern, title):
                    return district

    return None


def extract_district_from_content(content, title):
    """
    TrÃ­ch xuáº¥t quáº­n tá»« ná»™i dung náº¿u khÃ´ng tÃ¬m tháº¥y trong tiÃªu Ä‘á»
    """
    # Káº¿t há»£p ná»™i dung vÃ  tiÃªu Ä‘á» Ä‘á»ƒ tÃ¬m quáº­n
    combined_text = title + " " + content

    # TÃ¬m cÃ¡c quáº­n trong vÄƒn báº£n
    for district in SHENZHEN_DISTRICTS:
        if district in combined_text:
            return district

    # TÃ¬m theo tá»« khÃ³a Ä‘á»‹a danh
    for district, keywords in DISTRICT_KEYWORDS.items():
        for keyword in keywords:
            if keyword in combined_text:
                # Kiá»ƒm tra xem tá»« khÃ³a cÃ³ pháº£i lÃ  má»™t tá»« Ä‘á»™c láº­p khÃ´ng
                pattern = r'[^a-zA-Z0-9]' + re.escape(keyword) + r'[^a-zA-Z0-9]'
                if re.search(pattern, combined_text):
                    return district

    # Náº¿u váº«n khÃ´ng tÃ¬m tháº¥y, tráº£ vá» "æœªçŸ¥åŒº" (Quáº­n khÃ´ng xÃ¡c Ä‘á»‹nh)
    return "æœªçŸ¥åŒº"


def get_district(title, content):
    """
    Láº¥y quáº­n: Æ°u tiÃªn tiÃªu Ä‘á» trÆ°á»›c, sau Ä‘Ã³ Ä‘áº¿n ná»™i dung
    """
    # Æ¯u tiÃªn 1: TÃ¬m trong tiÃªu Ä‘á»
    district_from_title = extract_district_from_title(title)
    if district_from_title:
        print(f"  ğŸ¯ Quáº­n tÃ¬m tháº¥y trong tiÃªu Ä‘á»: {district_from_title}")
        return district_from_title

    # Æ¯u tiÃªn 2: TÃ¬m trong ná»™i dung
    district_from_content = extract_district_from_content(content, title)
    if district_from_content and district_from_content != "æœªçŸ¥åŒº":
        print(f"  ğŸ“„ Quáº­n tÃ¬m tháº¥y trong ná»™i dung: {district_from_content}")
        return district_from_content

    # KhÃ´ng tÃ¬m tháº¥y
    print("  âš ï¸  KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh quáº­n")
    return "æœªçŸ¥åŒº"


def parse_address_components(address_text):
    """
    PhÃ¢n tÃ­ch Ä‘á»‹a chá»‰ thÃ nh cÃ¡c thÃ nh pháº§n: å°åŒº, æ ‹æ•°, å•å…ƒ
    """
    # LÃ m sáº¡ch Ä‘á»‹a chá»‰ trÆ°á»›c
    address = address_text.strip()

    # --- BÆ¯á»šC 1: XÃ³a cÃ¡c tá»« khÃ³a thá»«a á»Ÿ Äáº¦U chuá»—i ---
    is_prefix_cleaned = True
    while is_prefix_cleaned:
        is_prefix_cleaned = False
        location_prefixes = ["æ·±åœ³å¸‚", "æ·±åœ³"]
        for location in location_prefixes:
            if address.startswith(location):
                address = address[len(location):].strip(' :ï¼š')
                is_prefix_cleaned = True
                break

        if not is_prefix_cleaned:
            for keyword in CLEANUP_KEYWORDS:
                if address.startswith(keyword):
                    address = address[len(keyword):].strip(' :ï¼š')
                    is_prefix_cleaned = True
                    break

    # --- BÆ¯á»šC 2: TÃ¡ch thÃ nh pháº§n Ä‘á»‹a chá»‰ ---
    xiaoqu = ""  # å°åŒº
    dongshu = ""  # æ ‹æ•°
    danyuan = ""  # å•å…ƒ

    # Máº«u regex Ä‘á»ƒ tÃ¡ch Ä‘á»‹a chá»‰
    patterns = [
        # Máº«u: "å°åŒºå" + "æ ‹/å·æ¥¼" + "å•å…ƒ"
        r'(.+?)(\d+[æ ‹å·æ¥¼])(\d*å•å…ƒ)?',
        # Máº«u: "å°åŒºå" + "æ ‹"
        r'(.+?)(\d+[æ ‹å·æ¥¼])',
        # Máº«u: "å°åŒºå" + "å•å…ƒ"
        r'(.+?)(\d*å•å…ƒ)',
        # Máº«u chá»‰ cÃ³ sá»‘ (cho cÃ¡c trÆ°á»ng há»£p Ä‘Æ¡n giáº£n)
        r'(\D+?)(\d+)'
    ]

    for pattern in patterns:
        match = re.search(pattern, address)
        if match:
            groups = match.groups()
            if len(groups) >= 2:
                xiaoqu = groups[0].strip()
                if 'æ ‹' in str(groups[1]) or 'å·æ¥¼' in str(groups[1]) or 'æ¥¼' in str(groups[1]):
                    dongshu = groups[1].strip()
                else:
                    # Náº¿u khÃ´ng pháº£i sá»‘ tÃ²a nhÃ , cÃ³ thá»ƒ lÃ  sá»‘ Ä‘Æ¡n vá»‹
                    danyuan = groups[1].strip()

                if len(groups) >= 3 and groups[2]:
                    danyuan = groups[2].strip()
                break

    # Náº¿u khÃ´ng tÃ¡ch Ä‘Æ°á»£c báº±ng regex, thá»­ phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n hÆ¡n
    if not xiaoqu:
        # TÃ¬m vá»‹ trÃ­ cá»§a cÃ¡c tá»« khÃ³a Ä‘á»‹a chá»‰
        building_keywords = ["æ ‹", "å·æ¥¼", "æ¥¼", "å¹¢"]
        unit_keywords = ["å•å…ƒ", "åº§"]

        building_pos = -1
        unit_pos = -1

        for keyword in building_keywords:
            pos = address.find(keyword)
            if pos != -1 and (building_pos == -1 or pos < building_pos):
                building_pos = pos

        for keyword in unit_keywords:
            pos = address.find(keyword)
            if pos != -1 and (unit_pos == -1 or pos < unit_pos):
                unit_pos = pos

        if building_pos != -1:
            xiaoqu = address[:building_pos].strip()
            if unit_pos != -1 and unit_pos > building_pos:
                dongshu = address[building_pos:unit_pos + 2].strip()  # +2 Ä‘á»ƒ láº¥y cáº£ tá»« khÃ³a
                danyuan = address[unit_pos:].strip()
            else:
                dongshu = address[building_pos:].strip()
        else:
            # Náº¿u khÃ´ng tÃ¬m tháº¥y tá»« khÃ³a, coi toÃ n bá»™ lÃ  tÃªn khu phá»‘
            xiaoqu = address

    # LÃ m sáº¡ch káº¿t quáº£
    xiaoqu = xiaoqu.strip(' ,ï¼Œ.ã€‚')
    dongshu = dongshu.strip(' ,ï¼Œ.ã€‚')
    danyuan = danyuan.strip(' ,ï¼Œ.ã€‚')

    print(f"  ğŸ˜ï¸ å°åŒº: '{xiaoqu}'")
    print(f"  ğŸ¢ æ ‹æ•°: '{dongshu}'")
    print(f"  ğŸšª å•å…ƒ: '{danyuan}'")

    return xiaoqu, dongshu, danyuan


def parse_title_hybrid_improved(title_text):
    """
    HÃ m phÃ¢n tÃ­ch tiÃªu Ä‘á» Ä‘Ã£ Ä‘Æ°á»£c cáº£i tiáº¿n cho website Guangdong
    """
    address = title_text.strip()

    # --- BÆ¯á»šC 1: XÃ³a cÃ¡c tá»« khÃ³a thá»«a á»Ÿ Äáº¦U chuá»—i ---
    is_prefix_cleaned = True
    while is_prefix_cleaned:
        is_prefix_cleaned = False
        location_prefixes = ["æ·±åœ³å¸‚", "æ·±åœ³"]
        for location in location_prefixes:
            if address.startswith(location):
                address = address[len(location):].strip(' :ï¼š')
                is_prefix_cleaned = True
                print(f"  ğŸ—‘ï¸ ÄÃ£ xÃ³a Ä‘á»‹a danh: '{location}'")
                break

        if not is_prefix_cleaned:
            for keyword in CLEANUP_KEYWORDS:
                if address.startswith(keyword):
                    address = address[len(keyword):].strip(' :ï¼š')
                    is_prefix_cleaned = True
                    print(f"  ğŸ§¹ ÄÃ£ xÃ³a tá»« khÃ³a: '{keyword}'")
                    break

    # --- BÆ¯á»šC 2: Cáº¯t bá» pháº§n Ä‘uÃ´i thá»«a ---
    min_pos = -1
    for keyword in CLEANUP_KEYWORDS:
        pos = address.find(keyword)
        if pos > 0 and (min_pos == -1 or pos < min_pos):
            min_pos = pos

    if min_pos != -1:
        address = address[:min_pos].strip()

    # --- BÆ¯á»šC 3: TÃ¬m vÃ  cáº¯t táº¡i tá»« khÃ³a Ä‘á»‹a chá»‰ ---
    address = address.replace('ï¼ˆæš‚åï¼‰', '').strip()
    address_keywords = ["å·", "å•å…ƒ", "å·æ¥¼", "æ¥¼", "å¼„", "å®…æ¥¼", "å¹¢", "è¡—é“", "å°åŒº", "ä¸­å­¦", "å°å­¦", "è¡—åŠ", "æ‘",
                        "è‹‘", "é™¢", "æ ‹", "èŠ±å›­"]
    found_pos = -1
    found_keyword = None

    for keyword in address_keywords:
        pos = address.rfind(keyword)
        if pos != -1:
            cut_position = pos + len(keyword)
            if cut_position > found_pos:
                found_pos = cut_position
                found_keyword = keyword

    if found_pos != -1 and found_keyword:
        address = address[:found_pos].strip()
        print(f"  âœ… ÄÃ£ cáº¯t táº¡i tá»« khÃ³a: '{found_keyword}'")

    return address


def read_checkpoint():
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            try:
                content = f.read().strip()
                return int(content) if content else 0
            except (ValueError, IndexError):
                return 0
    return 0


def write_checkpoint(page_num):
    with open(checkpoint_file, 'w') as f:
        f.write(str(page_num))


# --- PHáº¦N CHÃNH: THU THáº¬P VÃ€ Xá»¬ LÃ Dá»® LIá»†U ---
try:
    all_extracted_data = []
    all_raw_titles = []

    # --- Láº¥y tá»•ng sá»‘ trang má»™t cÃ¡ch tá»± Ä‘á»™ng ---
    print("Äang kiá»ƒm tra tá»•ng sá»‘ trang...")
    initial_payload = {
        "gdbsDivision": "440300",
        "gdbsOrgNum": "MB2C94128",
        "keywords": "ç”µæ¢¯å»ºè®¾",
        "page": 1,
        "position": "title",
        "range": "site",
        "recommand": 1,
        "service_area": 755,
        "site_id": "755016",
        "sort": "smart"
    }

    try:
        response = requests.post(api_url, headers=headers, json=initial_payload, timeout=30)
        response.raise_for_status()
        data = response.json()
        print(f"âœ… Káº¿t ná»‘i API thÃ nh cÃ´ng!")

        # FIXED: Correct data structure parsing
        if 'data' in data and 'news' in data['data']:
            results = data['data']['news']['list']
            total_items = data['data']['news'].get('total', 0)

            # Calculate total pages (assuming 20 items per page)
            total_pages = (total_items + 19) // 20  # Ceiling division

            print(f"ğŸ“ˆ Tá»•ng sá»‘ káº¿t quáº£: {total_items}")
            print(f"ğŸ“„ Tá»•ng sá»‘ trang: {total_pages}")
            print(f"ğŸ“ Sá»‘ má»¥c trÃªn trang 1: {len(results)}")

            if total_pages == 0:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ nÃ o vá»›i tá»« khÃ³a 'ç”µæ¢¯å»ºè®¾'")
                exit()
        else:
            print("âŒ Cáº¥u trÃºc dá»¯ liá»‡u khÃ´ng Ä‘Ãºng")
            print(f"ğŸ“‹ CÃ¡c keys cÃ³ trong data: {list(data.get('data', {}).keys())}")
            exit()

    except requests.exceptions.RequestException as e:
        print(f"âŒ Lá»—i káº¿t ná»‘i API: {e}")
        exit()
    except json.JSONDecodeError as e:
        print(f"âŒ Lá»—i phÃ¢n tÃ­ch JSON: {e}")
        exit()

    # --- Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh crawl ---
    last_completed_page = read_checkpoint()
    start_page = last_completed_page + 1

    if start_page > total_pages:
        print(f"âœ… Checkpoint cho tháº¥y Ä‘Ã£ thu tháº­p xong {last_completed_page}/{total_pages} trang.")
    else:
        if start_page > 1:
            print(f"ğŸ”„ Tiáº¿p tá»¥c tá»« trang {start_page}...")

        for page_num in range(start_page, total_pages + 1):
            payload = initial_payload.copy()
            payload['page'] = page_num

            print(f"--- Äang thu tháº­p trang {page_num}/{total_pages} ---")

            response = None
            for attempt in range(retry_attempts):
                try:
                    response = requests.post(api_url, headers=headers, json=payload, timeout=30)
                    response.raise_for_status()
                    data = response.json()
                    break
                except requests.exceptions.RequestException as req_err:
                    print(f"  âŒ Lá»—i káº¿t ná»‘i (láº§n {attempt + 1}/{retry_attempts}): {req_err}")
                    if attempt < retry_attempts - 1:
                        time.sleep(retry_delay)
                    else:
                        print("  ğŸš« ÄÃ£ háº¿t sá»‘ láº§n thá»­ láº¡i. Bá» qua trang nÃ y.")
                        continue

            if response is None:
                continue

            try:
                # FIXED: Correct data structure
                results = data.get('data', {}).get('news', {}).get('list', [])
            except:
                print(f"  âŒ KhÃ´ng thá»ƒ láº¥y dá»¯ liá»‡u tá»« trang {page_num}")
                continue

            if not results:
                print(f"â„¹ï¸ Trang {page_num} khÃ´ng cÃ³ dá»¯ liá»‡u.")
                break

            valid_items_count = 0
            for item in results:
                full_title = item.get('title', '')
                # Remove <em> tags from title
                full_title = re.sub(r'<.*?>', '', full_title)
                content = item.get('content', '')
                date_string = item.get('pub_time', '')
                url = item.get('url', '')

                # Lá»ŒC: Chá»‰ xá»­ lÃ½ náº¿u tiÃªu Ä‘á» chá»©a tá»« khÃ³a Ä‘á»‹a chá»‰
                if not any(keyword in full_title for keyword in ADDRESS_INDICATOR_KEYWORDS):
                    print(f"  â– Bá» qua (khÃ´ng cÃ³ tá»« khÃ³a Ä‘á»‹a chá»‰): {full_title[:50]}...")
                    continue

                # Xá»¬ LÃ
                if full_title and date_string:
                    all_raw_titles.append(full_title)

                    # TrÃ­ch xuáº¥t quáº­n (Æ°u tiÃªn tiÃªu Ä‘á» trÆ°á»›c)
                    district = get_district(full_title, content)

                    # LÃ m sáº¡ch tiÃªu Ä‘á» vÃ  phÃ¢n tÃ­ch Ä‘á»‹a chá»‰
                    cleaned_address = parse_title_hybrid_improved(full_title)
                    xiaoqu, dongshu, danyuan = parse_address_components(cleaned_address)

                    # PhÃ¢n tÃ­ch ngÃ y
                    if 'T' in date_string:
                        date_part = date_string.split('T')[0]
                        parts = date_part.split('-')
                    else:
                        parts = date_string.split('-')

                    year, month, day = (parts[0], parts[1], parts[2]) if len(parts) >= 3 else ("", "", "")

                    all_extracted_data.append({
                        'åŒº': district,
                        'å°åŒº': xiaoqu,
                        'æ ‹æ•°': dongshu,
                        'å•å…ƒ': danyuan,
                        'å¹´': year,
                        'æœˆ': month,
                        'æ—¥': day
                    })

                    valid_items_count += 1
                    print(f"  âœ… [{valid_items_count}] {district} - {xiaoqu} {dongshu} {danyuan}")

            print(f"  ğŸ“Š Trang {page_num}: {valid_items_count}/{len(results)} má»¥c há»£p lá»‡")
            write_checkpoint(page_num)
            time.sleep(request_delay)

        # --- Ghi dá»¯ liá»‡u ra file ---
        print(f"\nğŸ‰ Thu tháº­p hoÃ n táº¥t. Tá»•ng cá»™ng {len(all_extracted_data)} tiÃªu Ä‘á» há»£p lá»‡.\n")

        if all_raw_titles:
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(all_raw_titles) + '\n')
            print(f"ğŸ“„ ÄÃ£ ghi {len(all_raw_titles)} tiÃªu Ä‘á» vÃ o file '{txt_filename}'")

        if all_extracted_data:
            columns_order = ['åŒº', 'å°åŒº', 'æ ‹æ•°', 'å•å…ƒ', 'å¹´', 'æœˆ', 'æ—¥']
            new_df = pd.DataFrame(all_extracted_data)[columns_order]

            try:
                if not os.path.exists(excel_filename):
                    pd.DataFrame(columns=columns_order).to_excel(excel_filename, sheet_name=sheet_name_to_update,
                                                                 index=False)
                    print(f"ğŸ“ ÄÃ£ táº¡o file Excel má»›i: {excel_filename}")

                book = load_workbook(excel_filename)
                if sheet_name_to_update not in book.sheetnames:
                    book.create_sheet(sheet_name_to_update)
                sheet = book[sheet_name_to_update]

                last_data_row = find_last_row_with_data(sheet)
                start_row = last_data_row + 1

                if start_row <= 1:
                    for col_idx, header in enumerate(columns_order, 1):
                        sheet.cell(row=1, column=col_idx, value=header)
                    start_row = 2

                print(f"ğŸ“ Äang ghi {len(new_df)} dÃ²ng dá»¯ liá»‡u...")
                for i, row_data in new_df.iterrows():
                    current_row = start_row + i
                    for col_idx, col_name in enumerate(columns_order, 1):
                        sheet.cell(row=current_row, column=col_idx, value=row_data[col_name])

                book.save(excel_filename)
                print("âœ… HoÃ n táº¥t! File Excel Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t.")
            except Exception as ex:
                print(f"âŒ Lá»—i khi ghi file Excel: {ex}")
        else:
            print("â„¹ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡ Ä‘á»ƒ ghi vÃ o file Excel.")

except Exception as e:
    print(f"âŒ ÄÃ£ xáº£y ra lá»—i: {e}")
    import traceback

    traceback.print_exc()